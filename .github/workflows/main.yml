name: CI/CD workflow

#! on va ajouter des variables d'environnement ici.
# Bien sûr on pourrait aussi mettre ces variables dans les secrets du repo.
env:
  SERVER_IP: "ec2-51-21-115-3.eu-north-1.compute.amazonaws.com"
  SERVER_USER: "ubuntu"
  SERVER_PORT: "2222"

# On va gérer la concurrence des jobs afin de ne lancer qu'un workflow à la fois et annuler le workflow en cours si un nouveau workflow "CI/CD workflow" est lancé
concurrency:
  group: ${{ github.workflow }} # Groupe de concurrence basé sur le nom du workflow
  cancel-in-progress: true # Annule le workflow en cours si un nouveau workflow "CI/CD workflow" est lancé

# On va lancer le workflow lorsqu'un push est fait sur la branche main
on:
  push:
    branches:
      - main

# pour les jobs on va en faire un ou on va regrouper nos tests et un autre pour le deploiement:
jobs:
  # pour le job test on va mettre dedans:
  # verifier le code avec le script lint
  # Vérifier les vulnérabilités de nos dépendences avec la commande npm audit
  # Lancer les tests avec le script test:ci
  check-tests-backend:
    runs-on: ubuntu-latest
    # on précise le répertoire de travail pour ne pas le spécifier à chaque fois
    defaults:
      run:
        working-directory: ./server
    strategy: # Stratégie pour l'exécution du job
      fail-fast: true # Arrête tous les jobs si l'un d'entre eux échoue
      matrix: # Définit une matrice pour exécuter des jobs en parallèle
        # Les différentes commandes npm à exécuter
        npm_command:
          [
            "run test:ci",
            "run lint",
            "audit --production --audit-level=critical",
          ]
    steps:
      # on récupère notre repo
      - name: Checkout code
        uses: actions/checkout@v4
        # on installe node
      - name: Install node
        uses: actions/setup-node@v4
        # il y a une option pratique avec l'action setup-node:
        # On peut mettre en cache les dépendances pour gagner du temps
        # on précise que c'est pour npm
        # et on précise package-lock.json pour que le cache soit dépendant de ce fichier
        # on précise le chemin vers serveur car le workging directory par defautl n'est pas pris en compte dans le cache
        with:
          cache: "npm"
          cache-dependency-path: "./server/package-lock.json"
      # on installe les dépendances en mode clean install
      - name: Install dependencies
        run: npm ci
      # on lance les 3 commandes de la matrice
      - name: Lancer les tests
        run: npm ${{ matrix.npm_command }}
      - name: Upload coverage reports to Codecov
        uses: codecov/codecov-action@v5
        with:
          token: ${{ secrets.CODECOV_TOKEN }}

  check-frontend:
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: ./client
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      # Rappel: Le working-directory défini dans defaults fonctionne bien pour les commandes run, mais le cache de l'action setup-node nécessite toujours un chemin explicite vers le package-lock.json via cache-dependency-path.
      - name: Install node
        uses: actions/setup-node@v4
        with:
          cache: "npm"
          # Cette fois on va chercher le cache du côté de client
          cache-dependency-path: "./client/package-lock.json"
      - name: Install dependencies
        run: npm ci
      - name: Vérifie le code
        run: npm run lint
      - name: Vérifier les vulnérabilités
        run: npm audit --production --audit-level=critical

  # Nouveau job pour les tests E2E
  e2e-tests:
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: ./client
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Install node
        uses: actions/setup-node@v4
        with:
          cache: "npm"
          cache-dependency-path: "./client/package-lock.json"
      - name: Install dependencies
        run: npm ci
      # Installation de Cypress
      - name: Install Cypress
        uses: cypress-io/github-action@v6
        with:
          working-directory: ./client
          build: npm run build
          start: npm run preview
          wait-on: "http://localhost:4173"
          command: cypress run --e2e --config-file cypress.config.mjs
          # command: npm run test:e2e

  build-frontend:
    runs-on: ubuntu-latest
    needs: check-frontend
    defaults:
      run:
        working-directory: ./client
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Install node
        uses: actions/setup-node@v4
        with:
          cache: "npm"
          cache-dependency-path: "./client/package-lock.json"
      - name: Install dependencies
        run: npm ci
      - name: Build application frontend
        run: npm run build
      - name: Upload build frontend
        uses: actions/upload-artifact@v4
        with:
          # nom de l'artifact
          name: build-frontend
          # le workgin directory default n'a pas non plus été pris en comtpe dans cette action.
          # dist car c'est le dossier de build de notre application frontend.
          path: ./client/dist

  # pour le job deploy on va devoir:
  # Se connecter sur le vps avec notre clé ssh
  #  Récupérer le code source
  #  Installer les dépendances
  #  Lancer notre serveur avec pm2
  # On complexifiera le deploy quand il y aura l'application frontend
  deploy:
    # on veut que le deploy ne s'execute que si les tests sont passés
    needs: [check-tests-backend, build-frontend]
    runs-on: ubuntu-latest
    steps:
      # premiere chose on récupère le code source
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Download build frontend
        uses: actions/download-artifact@v4
        with:
          name: build-frontend
          # on le place dans dist à la racine comme server.
          # Si on regarde  server/index/js on peut voir que par rapport à server on revient en arrière ..
          # et on retourne dans dist.
          # app.get("*", (req, res) => {
          # res.sendFile(path.join(__dirname, "../dist/index.html"));
          # });
          # a noter que nous pourrions le mettre ailleurs pour le moment car nous sommes dans notre runner.github-token:
          # Il faudra bien le mettre à la racine au même niveau que server sur notre vps.
          path: ./dist
      # on va maintenant s'occuper du deploiement. On va faire plusieurs commandes dans la step
      - name: Deploy
        run: |
          # pour commencer on va activer l'agent ssh sur notre runner. En effet, l'agent ssh est un outil qui permet de stocker en mémoire notre clé ssh et l'utiliser à chaque fois que l'on veut se connecter à un serveur distant.
          # L'interêt est que l'on a pas besoin de stocker notre clé sur le disque dur du runner ni d'ajouter notre clé pour toutes les commandes ssh par la suite. L'agent ssh le fait automatiquement.
          # De plus la clé sera supprimée de la mémoire dès que le runner sera arrêté.
          # Pour lancer l'agent ssh, on va utiliser l'action ssh-agent puis on va ajouter notre clé ssh avec l'action ssh-add
          # eval permet de lancer l'agent ssh et -s permet de lancer l'agent ssh en mode compatible avec bash. On regroupe ssh-agent et le flag -s dans une parenthèse pour que ce soit considéré comme une commande unique.
          eval "$(ssh-agent -s)"

          # On va récupérer via le contexte secrets avec la clé SSH_KEY_VPS que l'on a ajoutée dans le repo le fin
          # ssh-add s'attends à recevoir un fichier ou une entrée standard. On va donc utiliser <<< pour envoyer la clé ssh en entrée standard et - permet de dire à ssh-add de lire de puis l'entrée standard
          ssh-add - <<< "${{ secrets.SSH_KEY_VPS }}"

          # Malgré l'utilisation de ssh-agent on  va créer un répertoire .ssh pour stocker le fingerprint (empreinte digitale)
          # du serveur. 
          # Cette empreinte est unique et permet de vérifier l'identité du serveur à chaque connexion comme une carte d'identité. C'est ce que l'on vous demande de valider lors de la première connexion ssh en général.
          # Cela sert à s'assurer que l'on se connecte bien à un serveur connu et non un serveur malveillant. Sinon on s'exploserait à une attaque man-in-the-middle ou on serait redirigé vers un autre serveur qui nous ferait croire que c'est le serveur original.
          # on placera dans ce repertoire les options ssh et notamment  le fichier known_hosts ou l'on stockera le fingerprint du vps.
          mkdir -p ~/.ssh

          # On va maintenant enregistrer le fingerprint du serveur dans le fichier known_hosts
          #! On peut aller voir sur notre machine en local et on verra que le fichier known_hosts est bien présent dans le répertoire .ssh
          #! on peut même l'ouvrir avec nano et on verra que les différents serveurs connus sont bien enregistrés.
          # ssh-keyscan permet de récupérer le fingerprint du serveur (en s'y connectant via son ip et port). 
          # -H pour hacher ce que l'on récupère et on redirige la sortie dans le fichier known_hosts sur le runner.
          ssh-keyscan -H -p ${{ env.SERVER_PORT }} ${{ env.SERVER_IP }} >> ~/.ssh/known_hosts

          # Copie des fichiers
          # Maintenant pour cloner le repertoire vers notre vps et faire un git clone il faudrait ajouter une clé ssh entre le vps et github.
          # Mais on va copier directement le repo récupéré sur le runner sur le vps avec la commande scp (sécure Copy Protocol).
          # Cette commande permet de copier des fichiers et repertoires via la connexion ssh.
          # Par contre c'est juste une copie et donc vu que l'on ne va pas copier la racine de notre repo qui contient le .git l'historique git ne sera pas conservé mais on en a pas besoin.
          # scp -P <portSSH> -r (flag récursif) ./server (Source: tous les fichiers  du dossier server ) <user>@<ip>:<path> (Destination: chemin vers le repertoire de destination)
          scp -P ${{ env.SERVER_PORT }} -r ./server ${{ env.SERVER_USER }}@${{ env.SERVER_IP }}:/home/ubuntu/
          # on va copier le build frontend
          scp -P ${{ env.SERVER_PORT }} -r ./dist ${{ env.SERVER_USER }}@${{ env.SERVER_IP }}:/home/ubuntu/

          # Installation des dépendances
          ssh -p ${{ env.SERVER_PORT }} ${{ env.SERVER_USER }}@${{ env.SERVER_IP }} "cd /home/ubuntu/server && npm ci --omit=dev"

          # Démarrage de l'application
          # On va maintenant lancer ou relancer le serveur (s'il est déjà démarré) avec la commande pm2 startOrRestart et precisant le fichier de configuration du serveur. On lui précise l'environnement de production pour que PM2 utilise la clé env_production dans le fichier ecosystem.config.js puis on save la configuration de PM2.
          ssh -p ${{ env.SERVER_PORT }} ${{ env.SERVER_USER }}@${{ env.SERVER_IP }} "cd /home/ubuntu/server && sudo pm2 startOrRestart ecosystem.config.js --env production && sudo pm2 save"
